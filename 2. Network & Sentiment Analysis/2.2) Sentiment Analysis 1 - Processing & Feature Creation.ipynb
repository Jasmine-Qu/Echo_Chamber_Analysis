{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 1.03 seconds)\n",
      "Writing emoji data to C:\\Users\\Ace\\.demoji\\codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "source": [
    "#core libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "import sys\n",
    "\n",
    "#vader\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import *\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "\n",
    "#Display Setting\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "#detect language\n",
    "from textblob import TextBlob  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df = pd.read_csv(\"submissions_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wsb_df['created_utc'] = pd.to_datetime(wsb_df['created_utc'])\n",
    "wsb_df['created_utc'] = wsb_df['created_utc'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "wsb_df.sort_values(by=\"created_utc\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wsb_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "## Useful functions\n",
    "def seconds_time(s):\n",
    "    seconds_set = \" \"+str(s%60)+\"sec\" if s%60!=0 else \"\"\n",
    "    s = s//60\n",
    "    \n",
    "    mins_set = \" \"+str(s%60)+\"min\" if s%60!=0 else \"\"\n",
    "    s = s//60\n",
    "    \n",
    "    hrs_set = \" \"+str(s%24)+\"hr\" if s%24!=0 else \"\"\n",
    "    s = s//24\n",
    "    \n",
    "    s = str(s)+\"days\" if s!=0 else \"\"\n",
    "    \n",
    "    return s+hrs_set+mins_set+seconds_set\n",
    " \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    " \n",
    "## Functions\n",
    "def language_detection(text):\n",
    "    return TextBlob(str(text)).detect_language()\n",
    " \n",
    "#Pre-Processing\n",
    " \n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    " \n",
    "def remove_specials(text):\n",
    "    text = text.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    return re.sub(r'[^a-zA-Z0-9 ]',r'',text)\n",
    " \n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    " \n",
    "def remove_punctuations(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "    return \" \".join(punt_removed)\n",
    " \n",
    "def remove_stopwords(text, lang='english'):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    return \" \".join(stopwords_removed)\n",
    " \n",
    "def remove_all(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    elif text == '[removed]':\n",
    "        return ''\n",
    "    elif len(text) <= 0:\n",
    "        return ''\n",
    "    else:\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_specials(text)\n",
    "        text = remove_whitespace(text)\n",
    "        text = remove_punctuations(text)\n",
    "        return text\n",
    "\n",
    "# Auxiliar functions\n",
    "from pyspark.sql.types import *\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    " \n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark(pandas_df):\n",
    "    from pyspark.sql import SQLContext\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "        struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlContext.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'] = data['title'].apply(\n",
    "        lambda x: remove_all(x)\n",
    ")\n",
    "\n",
    "def emoji_to_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for i in range(len(words)):\n",
    "        a = list(demoji.findall(words[i]).values())\n",
    "        try:\n",
    "            words[i] = a[0]\n",
    "        except:\n",
    "            pass\n",
    "    return \" \".join(words)\n",
    "\n",
    "data['title'] = data['title'].apply(\n",
    "        lambda x: emoji_to_words(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER - daily positive/negative sentiment\n",
    "def vader_sentiment(title):\n",
    "    if title == '':\n",
    "        return 0.0\n",
    "    else:\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        sentiment_score=0.0\n",
    "        try:\n",
    "            sentiment_score=sentiment_score+analyser.polarity_scores(title)['compound']\n",
    "        except TypeError:\n",
    "            sentiment_score=0.0\n",
    "        return sentiment_score\n",
    "\n",
    "def vader_sentiment_indicator(title):\n",
    "    if title == '':\n",
    "        return 0\n",
    "    else:\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        sentiment_score=0.0\n",
    "        try:\n",
    "            sentiment_score=sentiment_score+analyser.polarity_scores(title)['compound']\n",
    "            sentiment_score = 1\n",
    "        except TypeError:\n",
    "            sentiment_score=0\n",
    "        return sentiment_score\n",
    "\n",
    "def mean_list(data):\n",
    "    data = [i for i in data if i is not None]\n",
    "    if len(data) <= 0:\n",
    "        return 0.0\n",
    "    return np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER - daily positive/negative sentiment\n",
    "data['sentiment_score'] = data['title'].apply(\n",
    "        lambda x: vader_sentiment(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BULLISH/BEARISH SCORE - Sentiment Analysis Using Keywords\n",
    "#creating a list of words indicating bullish trend and  list of words indicating bearish trend\n",
    "bull_words = ['call', 'long', 'all in', 'moon', 'going up', \n",
    "             'rocket', 'buy', 'long term', 'green',\n",
    "             'to the moon','doubling down','dd',\n",
    "             'tendies','yolo','paper hands','jpow'\n",
    "             'andromeda','rocket ships','yoloed','rocket']\n",
    "\n",
    "bear_words = ['put', 'short', 'going down', \n",
    "             'drop', 'bear', 'sell', 'red',\n",
    "             'guh','stonks','diamond hands',\n",
    "             'btfd','bag holder','hold the line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_bullbear(text, word_list):\n",
    "    score = 0.0\n",
    "    for word in word_list:\n",
    "        if word in text:\n",
    "            score += 1.0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bull_scores'] = data['title'].apply(\n",
    "        lambda x: calculate_score_bullbear(x, bull_words)\n",
    ")\n",
    "data['bear_scores'] = data['title'].apply(\n",
    "        lambda x: calculate_score_bullbear(x, bear_words)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_convert(date):\n",
    "    try:\n",
    "        final_date = pd.to_datetime(date, errors='coerce').strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        final_date = None\n",
    "    return final_date\n",
    "data['Date'] = data['created_utc'].apply(\n",
    "        lambda x: date_convert(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['subreddit','Date'], ascending=[True,True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = data.groupby([\"Date\",'subreddit']) \\\n",
    "    .agg({\"score\":'mean',\n",
    "          \"sentiment_score\":'mean',\n",
    "          \"bull_scores\":'mean',\n",
    "          \"bear_scores\":'mean',\n",
    "          \"num_comments\":\"sum\",\n",
    "          \"num_crossposts\":\"sum\"\n",
    "         }).reset_index()\n",
    "data_group['Date'] = data_group['Date'].map({\n",
    "    date_i: pd.to_datetime(date_i) for date_i in data_group['Date'].unique()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>bull_scores</th>\n",
       "      <th>bear_scores</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>RobinHood</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014529</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>SecurityAnalysis</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.087800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>investing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>stocks</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043971</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>2475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3927</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>RobinHood</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.052422</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3928</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>SecurityAnalysis</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>-0.025517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3929</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>investing</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>0.068067</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>stocks</td>\n",
       "      <td>50.553571</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3931</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>227.060000</td>\n",
       "      <td>0.110652</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2465</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3932 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         subreddit       score  sentiment_score  bull_scores  \\\n",
       "0    2019-01-01         RobinHood    1.000000         0.014529     0.117647   \n",
       "1    2019-01-01  SecurityAnalysis    1.000000        -0.087800     0.000000   \n",
       "2    2019-01-01         investing    1.000000         0.050425     0.096154   \n",
       "3    2019-01-01            stocks    1.000000         0.014819     0.095238   \n",
       "4    2019-01-01    wallstreetbets    1.000000         0.043971     0.130000   \n",
       "...         ...               ...         ...              ...          ...   \n",
       "3927 2021-02-28         RobinHood    0.986111         0.052422     0.236111   \n",
       "3928 2021-02-28  SecurityAnalysis   56.666667        -0.025517     0.000000   \n",
       "3929 2021-02-28         investing   34.500000         0.068067     0.100000   \n",
       "3930 2021-02-28            stocks   50.553571         0.026812     0.089286   \n",
       "3931 2021-02-28    wallstreetbets  227.060000         0.110652     0.110000   \n",
       "\n",
       "      bear_scores  num_comments  num_crossposts  \n",
       "0        0.117647           155               0  \n",
       "1        0.500000            19               0  \n",
       "2        0.096154           948               0  \n",
       "3        0.095238           262               0  \n",
       "4        0.170000          2475               0  \n",
       "...           ...           ...             ...  \n",
       "3927     0.250000            12               0  \n",
       "3928     0.000000            16               1  \n",
       "3929     0.040000           997               0  \n",
       "3930     0.125000          2020               7  \n",
       "3931     0.100000          2465              14  \n",
       "\n",
       "[3932 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group.to_csv(\"Sentiment_Data_Subredditts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full_group = data.groupby([\"Date\"]) \\\n",
    "    .agg({\"score\":'mean',\n",
    "          \"sentiment_score\":'mean',\n",
    "          \"bull_scores\":'mean',\n",
    "          \"bear_scores\":'mean',\n",
    "          \"num_comments\":\"sum\",\n",
    "          \"num_crossposts\":\"sum\"\n",
    "         }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>bull_scores</th>\n",
       "      <th>bear_scores</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>3859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.087764</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>5218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.114198</td>\n",
       "      <td>0.117284</td>\n",
       "      <td>9197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.103152</td>\n",
       "      <td>0.111748</td>\n",
       "      <td>10356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029266</td>\n",
       "      <td>0.103679</td>\n",
       "      <td>0.137124</td>\n",
       "      <td>11137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>107.942598</td>\n",
       "      <td>0.077047</td>\n",
       "      <td>0.145015</td>\n",
       "      <td>0.093656</td>\n",
       "      <td>10524</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>68.073239</td>\n",
       "      <td>0.070610</td>\n",
       "      <td>0.149296</td>\n",
       "      <td>0.104225</td>\n",
       "      <td>29657</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>20.563014</td>\n",
       "      <td>0.051488</td>\n",
       "      <td>0.145205</td>\n",
       "      <td>0.098630</td>\n",
       "      <td>18596</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>2021-02-27</td>\n",
       "      <td>93.230065</td>\n",
       "      <td>0.075208</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.108497</td>\n",
       "      <td>210012</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>112.112245</td>\n",
       "      <td>0.072574</td>\n",
       "      <td>0.120408</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>5510</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>787 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       score  sentiment_score  bull_scores  bear_scores  \\\n",
       "0    2019-01-01    1.000000         0.038551     0.114583     0.140625   \n",
       "1    2019-01-02    1.000000         0.087764     0.040179     0.093750   \n",
       "2    2019-01-03    1.000000         0.045296     0.114198     0.117284   \n",
       "3    2019-01-04    1.000000         0.044735     0.103152     0.111748   \n",
       "4    2019-01-05    1.000000         0.029266     0.103679     0.137124   \n",
       "..          ...         ...              ...          ...          ...   \n",
       "782  2021-02-24  107.942598         0.077047     0.145015     0.093656   \n",
       "783  2021-02-25   68.073239         0.070610     0.149296     0.104225   \n",
       "784  2021-02-26   20.563014         0.051488     0.145205     0.098630   \n",
       "785  2021-02-27   93.230065         0.075208     0.117647     0.108497   \n",
       "786  2021-02-28  112.112245         0.072574     0.120408     0.114286   \n",
       "\n",
       "     num_comments  num_crossposts  \n",
       "0            3859               0  \n",
       "1            5218               0  \n",
       "2            9197               0  \n",
       "3           10356               0  \n",
       "4           11137               0  \n",
       "..            ...             ...  \n",
       "782         10524              10  \n",
       "783         29657              38  \n",
       "784         18596              17  \n",
       "785        210012              30  \n",
       "786          5510              22  \n",
       "\n",
       "[787 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_group.to_csv(\"Sentiment_Data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment tracking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
