{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#vader\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Display Setting\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "#detect language\n",
    "from textblob import TextBlob  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful functions\n",
    "def seconds_time(s):\n",
    "    seconds_set = \" \"+str(s%60)+\"sec\" if s%60!=0 else \"\"\n",
    "    s = s//60\n",
    "    \n",
    "    mins_set = \" \"+str(s%60)+\"min\" if s%60!=0 else \"\"\n",
    "    s = s//60\n",
    "    \n",
    "    hrs_set = \" \"+str(s%24)+\"hr\" if s%24!=0 else \"\"\n",
    "    s = s//24\n",
    "    \n",
    "    s = str(s)+\"days\" if s!=0 else \"\"\n",
    "    \n",
    "    return s+hrs_set+mins_set+seconds_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[2]\")\n",
    "spark = SparkSession.builder.master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of Comments data\n",
    "- author: author name\n",
    "- author_fullname: author ID\n",
    "- body: text of comments\n",
    "- created_utc: datetime the comment was created\n",
    "- id: comment id\n",
    "- link_id: submission id that the comment is under\n",
    "- parent_id: id of the comment that this comment is under\n",
    "- score: Not sure but keeping just in case its some special score\n",
    "- subreddit: subreddit name\n",
    "- subreddit_id: subreddit id\n",
    "- updated_utc: datetime the comment was last updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data done extract in  8min 56sec with count of 735120\n"
     ]
    }
   ],
   "source": [
    "start_time_df = datetime.datetime.now()\n",
    "\n",
    "#directory of the unzipped data profile\n",
    "df = spark.read.json(r\".\\*\\*\\comments\\*.json\")\n",
    "\n",
    "stop_time_df = datetime.datetime.now()\n",
    "print(\"Data done extract in \"+seconds_time((stop_time_df-start_time_df).seconds)+\" with count of \"+str(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set = df.withColumn(\"created_utc\", F.col(\"created_utc\")/1000)\n",
    "df_set = df_set.withColumn(\"created_utc\", F.from_unixtime(\"created_utc\"))\n",
    "df_set = df_set.orderBy(F.col(\"created_utc\").asc())\n",
    "df_set = df_set.dropDuplicates()\n",
    "df_set = df_set.orderBy(F.col(\"created_utc\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------------+--------------------+-------------------+-------+-------+-----------+---------+-------------+-----+--------------+------------+----------------+\n",
      "|              author|author_fullname|author_fullname_lvl|                body|        created_utc|     id|link_id|link_id_lvl|parent_id|parent_id_lvl|score|     subreddit|subreddit_id|subreddit_id_lvl|\n",
      "+--------------------+---------------+-------------------+--------------------+-------------------+-------+-------+-----------+---------+-------------+-----+--------------+------------+----------------+\n",
      "|           baitshop1|       2djs9bnu|                 t2|Just put on some ...|2019-01-01 08:00:01|ecy7pmk| ab67jz|         t3|   ab67jz|           t3|    7|wallstreetbets|       2th52|              t5|\n",
      "|             ssini92|          b1j9f|                 t2|But still not clo...|2019-01-01 08:00:05|ecy7prw| ab4cnf|         t3|  ecxuhwj|           t1|    1|wallstreetbets|       2th52|              t5|\n",
      "|             snaxks1|         11dqo1|                 t2|Depends on the IV...|2019-01-01 08:00:12|ecy7q1w| ab80fz|         t3|  ecy64pt|           t1|    1|     investing|       2qhhq|              t5|\n",
      "|              Towel4|          5m6h1|                 t2|1/25 here \n",
      "\n",
      "Don’t...|2019-01-01 08:00:16|ecy7q7s| ab7iw2|         t3|   ab7iw2|           t3|    2|wallstreetbets|       2th52|              t5|\n",
      "|   panties_in_my_ass|          zoe2z|                 t2|&gt; If you’re go...|2019-01-01 08:00:26|ecy7qnz| ab3sdq|         t3|  ecxlm4s|           t1|    1|     investing|       2qhhq|              t5|\n",
      "|       Shmokesshweed|        6lvmmbl|                 t2|       Small if true|2019-01-01 08:00:29|ecy7qrx| ab67jz|         t3|  ecy7oes|           t1|    6|wallstreetbets|       2th52|              t5|\n",
      "|        RestingPoint|          gg326|                 t2|It's okay. Game's...|2019-01-01 08:00:36|ecy7r2v| ab67jz|         t3|  ecy7mv2|           t1|    2|wallstreetbets|       2th52|              t5|\n",
      "|          fallstreet|       2pd1jott|                 t2|For indexes like ...|2019-01-01 08:00:39|ecy7r6x| ab6jqh|         t3|   ab6jqh|           t3|    1|     investing|       2qhhq|              t5|\n",
      "|           [deleted]|           None|               None|           [deleted]|2019-01-01 08:00:42|ecy7rb2| aax3sf|         t3|  ecwmtpr|           t1|    1|wallstreetbets|       2th52|              t5|\n",
      "|                Gkaz|          d0hdf|                 t2|The same one who'...|2019-01-01 08:00:48|ecy7rjw| ab7lei|         t3|  ecy2vrk|           t1|   37|wallstreetbets|       2th52|              t5|\n",
      "|           tinmaster|          ewkcs|                 t2|   Def worth a watch|2019-01-01 08:00:59|ecy7s1j| ab67jz|         t3|  ecy7mv2|           t1|    3|wallstreetbets|       2th52|              t5|\n",
      "|  sofa-king-wetarded|       2nm8kbb2|                 t2|     Average if true|2019-01-01 08:01:01|ecy7s3o| ab67jz|         t3|  ecy7qrx|           t1|    3|wallstreetbets|       2th52|              t5|\n",
      "|           [deleted]|           None|               None|           [deleted]|2019-01-01 08:01:10|ecy7sh9| ab67jz|         t3|  ecy786s|           t1|    1|wallstreetbets|       2th52|              t5|\n",
      "|       KarmaKingKong|          oksy3|                 t2|Bank of fucking A...|2019-01-01 08:01:16|ecy7srv| ab00yt|         t3|  ecxhdbn|           t1|    2|wallstreetbets|       2th52|              t5|\n",
      "|           [deleted]|           None|               None|           [removed]|2019-01-01 08:01:28|ecy7taj| ab76js|         t3|  ecy0ixj|           t1|    1|wallstreetbets|       2th52|              t5|\n",
      "|            Sosaille|          bylje|                 t2|imagine if trump ...|2019-01-01 08:01:33|ecy7th1| ab67jz|         t3|  ecy7ek1|           t1|    4|wallstreetbets|       2th52|              t5|\n",
      "|           tinmaster|          ewkcs|                 t2|We're going reddd...|2019-01-01 08:01:37|ecy7tnr| ab67jz|         t3|   ab67jz|           t3|    3|wallstreetbets|       2th52|              t5|\n",
      "|WeightLossMoneyGains|       2fhdt0z8|                 t2|Find another site...|2019-01-01 08:01:38|ecy7toz| ab67jz|         t3|  ecy7pl1|           t1|    1|wallstreetbets|       2th52|              t5|\n",
      "|            -Johnny-|          ebsj9|                 t2|About to load up ...|2019-01-01 08:01:47|ecy7u30| ab67jz|         t3|   ab67jz|           t3|    4|wallstreetbets|       2th52|              t5|\n",
      "|            deadjawa|          5uiwx|                 t2|&gt;it's just not...|2019-01-01 08:01:57|ecy7uh8| ab3jho|         t3|  ecy41gk|           t1|    3|     investing|       2qhhq|              t5|\n",
      "+--------------------+---------------+-------------------+--------------------+-------------------+-------+-------+-----------+---------+-------------+-----+--------------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data done extract in  3min 35sec\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "display(df_set.show())\n",
    "stop_time = datetime.datetime.now()\n",
    "print(\"Data done extract in \"+seconds_time((stop_time-start_time).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.toPandas().to_csv(\"comments_full.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
